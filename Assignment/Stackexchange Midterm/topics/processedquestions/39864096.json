{"is_answered": true, "view_count": 64, "tags": ["python", "pandas", "iteration", "large-files"], "last_activity_date": 1477357554, "answer_count": 3, "creation_date": 1475630795, "score": 3, "link": "http://stackoverflow.com/questions/39864096/what-is-the-most-efficient-way-to-compare-45-million-rows-of-text-file-to-about", "accepted_answer_id": 40229838, "owner": {"user_id": 6916973, "profile_image": "https://lh6.googleusercontent.com/-HaaAI166T-8/AAAAAAAAAAI/AAAAAAAALjI/GJMj2s8Y100/photo.jpg?sz=128", "user_type": "registered", "reputation": 25, "link": "http://stackoverflow.com/users/6916973/johnnyb", "display_name": "johnnyb"}, "title": "What is the most efficient way to compare 45 Million rows of Text File to about 200k rows text file and produce non matches from the smaller file?", "last_edit_date": 1475639334, "question_id": 39864096}